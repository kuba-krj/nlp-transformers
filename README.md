# Investigation of Transformers and self-attention mechanism
The aim of this project was to get familiar with the theory and practical aspects of Transformer models. 
The project consists of two parts: theoretical (maths) and coding. The details of the task can be found in 
the file <code>practical_4.pdf</code>. The report can be found in the file <code>practical4_report.pdf</code>.
## Theoretical part

In this section the main goal was toget some practice working with the self-attention equations 
and motivate why multi-headed self-attention can be preferable to single-headed self-attention.

## Coding part
In this section the task was to fill in the gaps in minGPT implementation and investigate gow the model can
learn some knowledge about the world during pretraining and access it in the finetuning part. 
More specifically, the task I was working on with the pretrained models was attempting to access the birth place of a notable
person, as written in their Wikipedia page. This is a particularly simple form of question answering. In the 
last part of the section I analysed the problem of hallucination of pre-trained large language models.

